{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ATS-HW05-ROCKET_Implementation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOV/hL1yYu301Nv5aIPMYTd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eTsN8R9Sb-i7"},"source":["# Submission information\n","This is a solution for Home Assignment 05 - Implementation of the ROCKET classification algorithm as can be seen in the article [\"ROCKET: Exceptionally fast and accurate time series classication using random convolutional kernels\" by Angus Dempster, Francois Petitjean, and Georey I. Webb](https://arxiv.org/pdf/1910.13051.pdf).\n","\n","Submitted by the following team members:\n","- Adi Yablonka\n","- Dana Averbuch\n","- Nati Ghatan\n"]},{"cell_type":"code","metadata":{"id":"vHZ6Zy1Ab3bh","executionInfo":{"status":"ok","timestamp":1616444599116,"user_tz":-120,"elapsed":4069,"user":{"displayName":"Nati Ghatan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijUz2D0WDjhvfCe2kQyPertLV_EMg3dS3lM7xMqg=s64","userId":"08323563140647242708"}}},"source":["# Import all necessary libraries\n","import math\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import math\n","\n","from sklearn.utils import shuffle\n","from sklearn.linear_model import RidgeClassifier\n","from tqdm import tqdm\n","from google.colab import drive"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLF1fJ2wfmM2","executionInfo":{"status":"ok","timestamp":1616444605253,"user_tz":-120,"elapsed":670,"user":{"displayName":"Nati Ghatan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijUz2D0WDjhvfCe2kQyPertLV_EMg3dS3lM7xMqg=s64","userId":"08323563140647242708"}}},"source":["# Define ROCKET model\n","class RocketNet(nn.Module):\n","    def __init__(self, data, class_labels, n_kernels, kernel_sizes):\n","        super(RocketNet, self).__init__()\n","        self.raw_data = data\n","        self.data = torch.max(data, torch.zeros_like(data))  # See section 3.2 in paper\n","        self.class_labels = class_labels\n","        self.n_samples = self.data.shape[0]\n","        self.signal_length = self.data.shape[1]\n","        self.n_kernels = n_kernels\n","        self.kernel_sizes = kernel_sizes\n","        self.__generate_random_kernels(self.n_kernels)\n","        self.classifier = None\n","\n","    def __generate_random_kernels(self, n_kernels: int, groups=1, stride=1):\n","        # Initialize variables\n","        self.kernels = []\n","        self.bias_terms = []\n","\n","        for kernel_index in range(n_kernels):\n","            # Stride: int = 1\n","            # Size: int\n","            current_kernel_size = np.random.choice(self.kernel_sizes)\n","\n","            # Dilation: int\n","            maximum_allowed_dilation = math.log2((self.signal_length - 1) / (current_kernel_size - 1))\n","            current_dilation = np.random.randint(low=0, high=maximum_allowed_dilation)\n","            dilation_factor = math.floor(math.pow(2, current_dilation))\n","\n","            # Padding: int\n","            current_padding_size = 0 if np.random.rand() <= 0.5 else \\\n","                int(((current_kernel_size - 1) * dilation_factor) / 2)\n","\n","            # Bias: bool\n","            current_bias = np.random.uniform(low=-1., high=1.)\n","\n","            # Create kernel with selected randomized parameters\n","            current_kernel = nn.Conv1d(in_channels=1,\n","                                       out_channels=1,\n","                                       kernel_size=current_kernel_size,\n","                                       stride=stride,\n","                                       groups=groups,\n","                                       dilation=dilation_factor,\n","                                       padding=current_padding_size,\n","                                       bias=False)\n","\n","            # Initialize kernel weights\n","            torch.nn.init.normal_(current_kernel.weight, mean=0.0, std=1.0)  # Draw from a Normal distribution\n","            current_kernel.weight.data = current_kernel.weight.data - current_kernel.weight.data.mean()  # Mean center\n","\n","            # Accumulate randomized kernel\n","            self.kernels.append(current_kernel)\n","            self.bias_terms.append(current_bias)\n","\n","    def compute_features_for_data(self, data):\n","        sample_features = []\n","        n_samples, signal_length = data.shape\n","        for sample_index in tqdm(range(n_samples)):\n","            # Reshape signal to meet model requirements, and feed it through the network\n","            current_signal = data[sample_index, :].view(1, signal_length)\n","            signal_features = self.forward(signal=current_signal)\n","\n","            max_value = [x.max().item() for x in signal_features]\n","            ppv = [x[x > 0].shape[-1] / x.shape[-1] for x in signal_features]\n","            sample_features.append(max_value + ppv)\n","\n","        return sample_features\n","\n","    def train(self, alpha=1.0, tolerance=1e-3):\n","        # Transform data through convolution kernels\n","        features = self.compute_features_for_data(self.data)\n","\n","        # Perform learning (Ridge regression)\n","        self.classifier = RidgeClassifier(alpha=alpha, tol=tolerance, normalize=False)\n","        self.classifier.fit(X=features, y=self.class_labels)\n","        print('Training score', self.classifier.score(X=features, y=self.class_labels))\n","\n","    def predict(self, data, labels=None):\n","        features = self.compute_features_for_data(data)\n","        print('Test score', self.classifier.score(X=features, y=labels))\n","\n","    def forward(self, signal):\n","        conv_output = []\n","        n_samples, signal_length = signal.shape\n","        for kernel, bias in zip(self.kernels, self.bias_terms):\n","            reshaped_signal = signal.view(n_samples, 1, signal_length)\n","            conv_output.append(kernel(reshaped_signal) + bias)\n","        return conv_output\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eVNcq-hf5Kf","executionInfo":{"status":"ok","timestamp":1616444823079,"user_tz":-120,"elapsed":670,"user":{"displayName":"Nati Ghatan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijUz2D0WDjhvfCe2kQyPertLV_EMg3dS3lM7xMqg=s64","userId":"08323563140647242708"}}},"source":["## Define running parameters\n","# Convolution\n","number_of_kernels = 500\n","permitted_kernel_sizes = [7, 9, 11]  # Taken from the ROCKET article\n","\n","# Classification\n","alpha = 1.0  # Regularization strength\n","tolerance = 1e-3  # Precision of the solution"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITfT42CZiMof","executionInfo":{"status":"ok","timestamp":1616444824994,"user_tz":-120,"elapsed":969,"user":{"displayName":"Nati Ghatan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijUz2D0WDjhvfCe2kQyPertLV_EMg3dS3lM7xMqg=s64","userId":"08323563140647242708"}},"outputId":"85be46ea-59a7-4f0c-96d7-568f9d9befd1"},"source":["## Load datasets\n","drive.mount('/content/drive/')\n","path_to_train = \"/content/drive/MyDrive/Colab Notebooks/ElectricDevices_TRAIN.tsv\"\n","path_to_test = \"/content/drive/MyDrive/Colab Notebooks/ElectricDevices_TEST.tsv\"\n","\n","def load_data_from_file(path_to_file, do_shuffle=False):\n","  data = pd.read_csv(path_to_file, header=None, sep='\\t')\n","  if do_shuffle:\n","    data = shuffle(data)\n","  data = torch.tensor(data.values.astype(np.float32))\n","  \n","  # Separate between class labels (first column) and actual data (rest of the columns)\n","  class_labels = data[:, 0]\n","  data = data[:, 1:]\n","\n","  return data, class_labels\n","\n","# Training data\n","train_data, train_class_labels = load_data_from_file(path_to_file=path_to_train,\n","                                 do_shuffle=True)\n","\n","\n","# Acquire data signal length\n","signal_length = train_data.shape[1]\n","\n","# Test data\n","test_data, test_class_labels = load_data_from_file(path_to_file=path_to_test)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bgVtfLsQj7RT","executionInfo":{"status":"ok","timestamp":1616444827336,"user_tz":-120,"elapsed":634,"user":{"displayName":"Nati Ghatan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijUz2D0WDjhvfCe2kQyPertLV_EMg3dS3lM7xMqg=s64","userId":"08323563140647242708"}}},"source":["# Define ROCKET network model\n","net = RocketNet(data=train_data,\n","                class_labels=train_class_labels,\n","                n_kernels=number_of_kernels,\n","                kernel_sizes=permitted_kernel_sizes)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rmmT9y03kntE","executionInfo":{"status":"ok","timestamp":1616444845571,"user_tz":-120,"elapsed":16217,"user":{"displayName":"Nati Ghatan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijUz2D0WDjhvfCe2kQyPertLV_EMg3dS3lM7xMqg=s64","userId":"08323563140647242708"}},"outputId":"78839e05-7f92-400e-cd29-4b9a4023e93e"},"source":["## Verify that network is functioning properly\n","def __compute_expected_output_size(signal_length, kernel_size, padding, dilation, stride):\n","    # Source: https://arxiv.org/pdf/1603.07285.pdf , Page 28\n","    nominator = (signal_length + (2 * padding) - kernel_size - ((kernel_size - 1) * (dilation - 1)))\n","    return math.floor(nominator / stride) + 1\n","\n","# Push training data through the network to get convolution results\n","results = net.forward(signal=train_data)\n","\n","# Validate output sizes\n","for kernel_index in range(net.n_kernels):\n","    current_kernel = net.kernels[kernel_index]\n","    kernel_size = current_kernel.kernel_size[0]\n","    observed_output_size = results[kernel_index].shape[-1]\n","    padding_size = current_kernel.padding[0]\n","    stride_size = current_kernel.stride[0]\n","    dilation_size = current_kernel.dilation[0]\n","    expected_output_size = __compute_expected_output_size(signal_length=signal_length,\n","                                                          kernel_size=kernel_size,\n","                                                          padding=padding_size,\n","                                                          dilation=dilation_size,\n","                                                          stride=stride_size)\n","    assert expected_output_size == observed_output_size\n","print(\"Result sizes validated successfully!\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Result sizes validated successfully!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZYrNL0lk58H","executionInfo":{"status":"ok","timestamp":1616445290990,"user_tz":-120,"elapsed":443607,"user":{"displayName":"Nati Ghatan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijUz2D0WDjhvfCe2kQyPertLV_EMg3dS3lM7xMqg=s64","userId":"08323563140647242708"}},"outputId":"8b304718-dea3-40d4-88be-84670134e61b"},"source":["## Train the network\n","net.train(alpha=alpha, tolerance=tolerance)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["100%|██████████| 8926/8926 [07:21<00:00, 20.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Training score 0.8909926058704907\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKzWZfzPlEve","executionInfo":{"status":"ok","timestamp":1616445680082,"user_tz":-120,"elapsed":383973,"user":{"displayName":"Nati Ghatan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijUz2D0WDjhvfCe2kQyPertLV_EMg3dS3lM7xMqg=s64","userId":"08323563140647242708"}},"outputId":"5b5eae24-28b8-4150-ad2a-8df08e3d56f3"},"source":["## Test the network\n","net.predict(data=test_data, labels=test_class_labels)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["100%|██████████| 7711/7711 [06:22<00:00, 20.13it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Test score 0.6350667877058748\n"],"name":"stdout"}]}]}